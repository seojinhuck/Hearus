# -*- coding: utf-8 -*-
"""DistilBert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G-33C1yXdPXISm-KzITt2wz-uLfYbuA9
"""



import torch
from transformers import DistilBertModel, DistilBertConfig

def prune_model(model, pruning_rate=0.1):
    # 모든 선형 계층에 대해 반복
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            # 가중치의 절대값을 기준으로 pruning_rate 비율 만큼 가중치를 제거
            threshold = torch.quantile(torch.abs(module.weight.data), pruning_rate)
            mask = torch.abs(module.weight.data) > threshold
            module.weight.data *= mask.float()

# DistilBERT 모델 로드
distilbert_model = DistilBertModel(DistilBertConfig())

# Pruning을 적용할 비율 설정
pruning_rate = 0.1

# Pruning 함수 호출
prune_model(distilbert_model, pruning_rate)

#DistilBERT는 이미 경량화된 모델이므로, Pruning을 적용할 때는 성능 저하에 주의

