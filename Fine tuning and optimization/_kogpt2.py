# -*- coding: utf-8 -*-
"""kogpt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lcXBNrD3VhRRtjGkonmq2kHG4tytQM0i
"""

!pip install transformers torch
!pip install konlpy scikit-learn
!pip install transformers[torch] accelerate -U
!pip install kobert-transformers
!pip install bert-extractive-summarizer
!pip install transformers --upgrade
!pip install scikit-learn

from google.colab import drive
drive.mount('/content/drive')

import os
import json
from konlpy.tag import Okt
from collections import Counter

# 한국어 불용어 목록 정의 (예시)
stopwords = set(["사용자가 정의"])

# 텍스트 정제 및 토큰화 함수
def tokenize_text(text):
    okt = Okt()
    # 정규 표현식 등을 사용한 추가적인 텍스트 정제가 필요할 수 있음
    tokens = okt.morphs(text, stem=True)  # 형태소 분석
    tokens = [word for word in tokens if not word in stopwords]  # 불용어 제거
    return tokens

# 정수 인코딩 함수
def integer_encode_texts(tokenized_texts):
    words = sum(tokenized_texts, [])
    vocab = Counter(words)
    vocab = vocab.most_common(len(vocab))
    word_to_index = {word[0]: index + 1 for index, word in enumerate(vocab)}
    encoded_texts = [[word_to_index[word] for word in text] for text in tokenized_texts]
    return encoded_texts, word_to_index

# 빈 샘플 제거 함수
def remove_empty_samples(tokenized_texts):
    return [text for text in tokenized_texts if len(text) > 0]

# 파일 경로 설정 및 데이터 로드
directory_path = '데이터 경로 '
json_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.json')]

def load_data(json_files):
    texts = []
    for file_path in json_files:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
            texts.append(data['06_transcription']['1_text'])
    return texts

# JSON 파일에서 텍스트 데이터 로드 및 전처리
texts = load_data(json_files)
tokenized_texts = [tokenize_text(text) for text in texts]
tokenized_texts = remove_empty_samples(tokenized_texts)
encoded_texts, word_to_index = integer_encode_texts(tokenized_texts)

import json
import torch
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from transformers import GPT2Tokenizer

model_name = 'skt/kogpt2-base-v2'

tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,
                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',
                                                    pad_token='<pad>', mask_token='<mask>')
model = GPT2LMHeadModel.from_pretrained(model_name)

def convert_to_string(encoded_texts, word_to_index):
    index_to_word = {i: word for word, i in word_to_index.items()}
    return [' '.join([index_to_word.get(word, '') for word in text]) for text in encoded_texts]

texts_str = convert_to_string(encoded_texts, word_to_index)

# 각 텍스트를 개별적으로 처리하여 리스트에 저장
encoded_texts_kogpt = [tokenizer.encode_plus(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt') for text in texts_str]

# 학습 및 검증 세트로 분할
train_encodings, val_encodings = train_test_split(encoded_texts_kogpt, test_size=0.2)

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings)

    def __getitem__(self, idx):
        # 각 토큰화된 결과에서 텐서를 추출하고 차원을 조절.
        item = {key: val.squeeze(0) for key, val in self.encodings[idx].items()}
        item['labels'] = item['input_ids'].clone()
        return item


train_dataset = MyDataset(train_encodings)
val_dataset = MyDataset(val_encodings)



training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=5,              # 모델이 얼마나 데이터를 보는지
    per_device_train_batch_size=4,  # 계산효율성 , 경사하강 , 과적합감소
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    evaluation_strategy='epoch',
    max_steps=1000
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()

evaluation_results = trainer.evaluate()
print(evaluation_results)

# 하이퍼파라미터 조정 예시 (학습률 조정)
training_args.learning_rate = 5e-5

# 새로운 학습 인자로 트레이너 재설정
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# 모델 재학습
trainer.train()

model.save_pretrained('/content/drive/MyDrive/Model/testfile')

from transformers import PreTrainedTokenizerFast, AutoModelWithLMHead
import torch

# 토크나이저와 모델 불러오기
tokenizer = PreTrainedTokenizerFast.from_pretrained(
    "skt/kogpt2-base-v2",
    bos_token='</s>', eos_token='</s>', unk_token='<unk>',
    pad_token='<pad>', mask_token='<mask>'
)

# 학습된 모델 불러오기
model = AutoModelWithLMHead.from_pretrained('/content/drive/MyDrive/Model/testfile')

# 텍스트 프롬프트 설정
text = "문제를 풀어볼게요 오늘 강의에서 요통이 뭔지가 제일 중요합니다"

# 입력 텍스트를 인코딩
input_ids = tokenizer.encode(text, return_tensors="pt")

# CUDA 사용 가능한 경우 GPU로 모델을 옮김
if torch.cuda.is_available():
    model = model.to('cuda')
    input_ids = input_ids.to('cuda')

# 텍스트 생성
gen_ids = model.generate(
    input_ids,
    max_length=160,
    repetition_penalty=3.0,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
    bos_token_id=tokenizer.bos_token_id,
    use_cache=True
)

# 생성된 텍스트 디코딩
generated = tokenizer.decode(gen_ids[0, :].tolist(), skip_special_tokens=True)
print(generated)



